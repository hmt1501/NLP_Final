{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45dcb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '44d441cc99e5a3b8eaa6eff95a6dbd13b53de6a6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b97482a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49590de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[02:13:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Logger initialized. Log file:                  \u001b]8;id=594990;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=40020;file:///home/thanhnx/bhc/utils/logger.py#150\u001b\\\u001b[2m150\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[35m/home/thanhnx/bhc/logs/\u001b[0m\u001b[95mtraining.log\u001b[0m            \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m[02:13:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m =============== BHC Medical Summarization      \u001b]8;id=186451;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=673022;file:///home/thanhnx/bhc/utils/logger.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         Fine-tuning ================                   \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Model: Qwen/Qwen3-\u001b[1;36m1.\u001b[0m7B                \u001b]8;id=985735;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=758169;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#497\u001b\\\u001b[2m497\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Output: output/qwen_finetune          \u001b]8;id=690888;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=142526;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#498\u001b\\\u001b[2m498\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Started: \u001b[1;36m2025\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m09\u001b[0m \u001b[1;92m02:13:35\u001b[0m          \u001b]8;id=762697;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=657595;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#499\u001b\\\u001b[2m499\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Data path:                            \u001b]8;id=48841;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=610006;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#501\u001b\\\u001b[2m501\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         dataset/mimic-iv-note-ext-di-bhc/data \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         set                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using device: cuda                    \u001b]8;id=949640;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=148215;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#505\u001b\\\u001b[2m505\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1mTraining Configuration:\u001b[0m                        \u001b]8;id=520263;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=846288;file:///home/thanhnx/bhc/utils/logger.py#188\u001b\\\u001b[2m188\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   LoRA Rank: \u001b[1;36m8\u001b[0m                                 \u001b]8;id=331877;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=32428;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   LoRA Alpha: \u001b[1;36m8\u001b[0m                                \u001b]8;id=854190;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=416228;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   LoRA Dropout: \u001b[1;36m0.1\u001b[0m                            \u001b]8;id=246154;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=634137;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Target Modules: \u001b[1m[\u001b[0m\u001b[32m'q_proj'\u001b[0m, \u001b[32m'k_proj'\u001b[0m,         \u001b]8;id=783575;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=399547;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[32m'v_proj'\u001b[0m, \u001b[32m'o_proj'\u001b[0m\u001b[1m]\u001b[0m                            \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Learning Rate: \u001b[1;36m0.0005\u001b[0m                        \u001b]8;id=495560;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=775702;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Epochs: \u001b[1;36m30\u001b[0m                                   \u001b]8;id=787384;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=138694;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Batch Size: \u001b[1;36m4\u001b[0m                                \u001b]8;id=388590;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=110818;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Gradient Accumulation: \u001b[1;36m16\u001b[0m                    \u001b]8;id=991800;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=493488;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhoangmanhtan1501\u001b[0m (\u001b[33mhoangmanhtan1501-neu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m setting up run pbdegmhr (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run pbdegmhr (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run pbdegmhr (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run pbdegmhr (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/thanhnx/bhc/wandb/run-20251209_021336-pbdegmhr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlikely-star-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/hoangmanhtan1501-neu/mimic-iv-note-di-bhc_Qwen3-1.7B_finetuning\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hoangmanhtan1501-neu/mimic-iv-note-di-bhc_Qwen3-1.7B_finetuning/runs/pbdegmhr\u001b[0m\n",
      "\u001b[2;36m[02:13:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Wandb initialized:                    \u001b]8;id=720929;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=393183;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#550\u001b\\\u001b[2m550\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         mimic-iv-note-di-bhc_Qwen3-\u001b[1;36m1.\u001b[0m7B_finet \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         uning                                 \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading model and tokenizer\u001b[33m...\u001b[0m        \u001b]8;id=399880;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=246528;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#555\u001b\\\u001b[2m555\u001b[0m\u001b]8;;\u001b\\\n",
      "loading configuration file config.json from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6144,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.\n",
      "loading weights file model.safetensors from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/model.safetensors.index.json\n",
      "Instantiating Qwen3ForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `torch.int8` for 8-bit BnB quantization\n",
      "\u001b[2;36m[02:13:39]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m We will use \u001b[1;36m90\u001b[0m% of the memory on device \u001b[1;36m0\u001b[0m    \u001b]8;id=808844;file:///home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/accelerate/utils/modeling.py\u001b\\\u001b[2mmodeling.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=310503;file:///home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/accelerate/utils/modeling.py#987\u001b\\\u001b[2m987\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         for storing the model, and \u001b[1;36m10\u001b[0m% for the       \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         buffer to avoid OOM. You can set             \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         `max_memory` in to a higher value to use     \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         more memory \u001b[1m(\u001b[0mat your own risk\u001b[1m)\u001b[0m.              \u001b[2m               \u001b[0m\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.29it/s]\n",
      "loading configuration file generation_config.json from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside Qwen/Qwen3-1.7B.\n",
      "loading file vocab.json from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/vocab.json\n",
      "loading file merges.txt from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/merges.txt\n",
      "loading file tokenizer.json from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[2;36m[02:13:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Model and tokenizer loaded            \u001b]8;id=479495;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=459996;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#572\u001b\\\u001b[2m572\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         successfully!                         \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading datasets\u001b[33m...\u001b[0m                   \u001b]8;id=553293;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=49130;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#588\u001b\\\u001b[2m588\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   train:                              \u001b]8;id=785278;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=327614;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#590\u001b\\\u001b[2m590\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         dataset/mimic-iv-note-ext-di-bhc/data \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         set/train_4000_600_chars_251-350_pt.j \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         son                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   validation:                         \u001b]8;id=60655;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=331359;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#590\u001b\\\u001b[2m590\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         dataset/mimic-iv-note-ext-di-bhc/data \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         set/valid_4000_600_chars.json         \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   test:                               \u001b]8;id=838342;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=669537;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#590\u001b\\\u001b[2m590\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         dataset/mimic-iv-note-ext-di-bhc/data \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         set/test_4000_600_chars_last_100.json \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:13:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1mDataset Statistics:\u001b[0m                            \u001b]8;id=957026;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=32407;file:///home/thanhnx/bhc/utils/logger.py#188\u001b\\\u001b[2m188\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Training: \u001b[1;36m100\u001b[0m                                \u001b]8;id=858601;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=664709;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Validation: \u001b[1;36m2608\u001b[0m                             \u001b]8;id=464840;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=129410;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Test: \u001b[1;36m100\u001b[0m                                    \u001b]8;id=363176;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=142147;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Truncating texts to fit context       \u001b]8;id=68605;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=317654;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#635\u001b\\\u001b[2m635\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         window\u001b[33m...\u001b[0m                             \u001b[2m                      \u001b[0m\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 846.51 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2608/2608 [00:02<00:00, 924.70 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 933.33 examples/s]\n",
      "\u001b[2;36m[02:13:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1mTruncation Summary:\u001b[0m                            \u001b]8;id=153336;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=687345;file:///home/thanhnx/bhc/utils/logger.py#188\u001b\\\u001b[2m188\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Train truncated: \u001b[1;36m0\u001b[0m/\u001b[1;36m100\u001b[0m                       \u001b]8;id=807903;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=117903;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Val truncated: \u001b[1;36m0\u001b[0m/\u001b[1;36m2608\u001b[0m                        \u001b]8;id=591305;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=374679;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   Test truncated: \u001b[1;36m0\u001b[0m/\u001b[1;36m100\u001b[0m                        \u001b]8;id=306803;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=334417;file:///home/thanhnx/bhc/utils/logger.py#190\u001b\\\u001b[2m190\u001b[0m\u001b]8;;\u001b\\\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 151669. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "PyTorch: setting up devices\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Preprocessing datasets\u001b[33m...\u001b[0m             \u001b]8;id=359511;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=321027;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#699\u001b\\\u001b[2m699\u001b[0m\u001b]8;;\u001b\\\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 22500.42 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2608/2608 [00:00<00:00, 37086.15 examples/s]\n",
      "\u001b[2;36m[02:13:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Selected \u001b[1;36m5\u001b[0m samples for per-epoch      \u001b]8;id=838344;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=906671;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#706\u001b\\\u001b[2m706\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         inference                             \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:13:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m EvaluationCallback initialized with \u001b[1;36m5\u001b[0m \u001b]8;id=924815;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=564919;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#106\u001b\\\u001b[2m106\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         inference samples                     \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m BestCheckpointCallback tracking       \u001b]8;id=910376;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=411804;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#235\u001b\\\u001b[2m235\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         eval_loss \u001b[1m(\u001b[0m\u001b[33mgreater_is_better\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m   \u001b[2m                      \u001b[0m\n",
      "/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:110: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Adding EOS to train dataset: 100%|‚ñà‚ñà| 100/100 [00:00<00:00, 23040.56 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 1084.11 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 48528.34 examples/s]\n",
      "Adding EOS to eval dataset: 100%|‚ñà| 2608/2608 [00:00<00:00, 50211.82 examples/s]\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 2608/2608 [00:02<00:00, 1055.46 examples/s]\n",
      "Truncating eval dataset: 100%|‚ñà‚ñà‚ñà| 2608/2608 [00:00<00:00, 124793.16 examples/s]\n",
      "\u001b[2;36m[02:13:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m ========================= Starting Training    \u001b]8;id=288389;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=256787;file:///home/thanhnx/bhc/utils/logger.py#176\u001b\\\u001b[2m176\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         ==========================                     \u001b[2m             \u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "The following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, summary, truncated. If text, summary, truncated are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 60\n",
      "  Number of trainable parameters = 3,211,264\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training started                      \u001b]8;id=776646;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=935518;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#290\u001b\\\u001b[2m290\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Running initial evaluation \u001b[1m(\u001b[0mepoch \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m  \u001b]8;id=31244;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=98246;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#115\u001b\\\u001b[2m115\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         before training\u001b[33m...\u001b[0m                    \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generating \u001b[1;36m5\u001b[0m sample predictions at    \u001b]8;id=588508;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=208496;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#156\u001b\\\u001b[2m156\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         step \u001b[1;36m0\u001b[0m\u001b[33m...\u001b[0m                             \u001b[2m                      \u001b[0m\n",
      "/home/thanhnx/bhc/summarization/fine_tune_llama.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "- `top_k`: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[2;36m[02:14:18]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m1\u001b[0m prediction generated         \u001b]8;id=231148;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=471029;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1558\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:14:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m2\u001b[0m prediction generated         \u001b]8;id=6814;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=795667;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1616\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:15:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m3\u001b[0m prediction generated         \u001b]8;id=356778;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=291369;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1644\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:15:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m4\u001b[0m prediction generated         \u001b]8;id=107175;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=97251;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1553\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:15:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m5\u001b[0m prediction generated         \u001b]8;id=360663;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=633052;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1479\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:16:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Logged \u001b[1;36m5\u001b[0m sample predictions to wandb  \u001b]8;id=562275;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=130889;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#208\u001b\\\u001b[2m208\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         at step \u001b[1;36m0\u001b[0m                             \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Saved predictions to                  \u001b]8;id=869693;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=659176;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#216\u001b\\\u001b[2m216\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         output/qwen_finetune/predictions_step \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         _0.json                               \u001b[2m                      \u001b[0m\n",
      "  0%|                                                    | 0/60 [00:00<?, ?it/s]The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, summary, truncated. If text, summary, truncated are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2608\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                   | 0/652 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|‚ñè                                          | 2/652 [00:00<03:33,  3.04it/s]\u001b[A\n",
      "  0%|‚ñè                                          | 3/652 [00:01<04:53,  2.21it/s]\u001b[A\n",
      "  1%|‚ñé                                          | 4/652 [00:01<05:26,  1.99it/s]\u001b[A\n",
      "  1%|‚ñé                                          | 5/652 [00:02<05:42,  1.89it/s]\u001b[A\n",
      "  1%|‚ñç                                          | 6/652 [00:03<05:47,  1.86it/s]\u001b[A\n",
      "  1%|‚ñç                                          | 7/652 [00:03<05:48,  1.85it/s]\u001b[A\n",
      "  1%|‚ñå                                          | 8/652 [00:04<05:47,  1.85it/s]\u001b[A\n",
      "  1%|‚ñå                                          | 9/652 [00:04<05:47,  1.85it/s]\u001b[A\n",
      "  2%|‚ñã                                         | 10/652 [00:05<05:43,  1.87it/s]\u001b[A\n",
      "  2%|‚ñã                                         | 11/652 [00:05<05:35,  1.91it/s]\u001b[A\n",
      "  2%|‚ñä                                         | 12/652 [00:06<05:28,  1.95it/s]\u001b[A\n",
      "  2%|‚ñä                                         | 13/652 [00:06<05:19,  2.00it/s]\u001b[A\n",
      "  2%|‚ñâ                                         | 14/652 [00:07<05:11,  2.05it/s]\u001b[A\n",
      "  2%|‚ñâ                                         | 15/652 [00:07<05:01,  2.11it/s]\u001b[A\n",
      "  2%|‚ñà                                         | 16/652 [00:07<04:53,  2.17it/s]\u001b[A\n",
      "  3%|‚ñà                                         | 17/652 [00:08<04:48,  2.20it/s]\u001b[A\n",
      "  3%|‚ñà‚ñè                                        | 18/652 [00:08<04:43,  2.24it/s]\u001b[A\n",
      "  3%|‚ñà‚ñè                                        | 19/652 [00:09<04:38,  2.27it/s]\u001b[A\n",
      "  3%|‚ñà‚ñé                                        | 20/652 [00:09<04:34,  2.30it/s]\u001b[A\n",
      "  3%|‚ñà‚ñé                                        | 21/652 [00:10<04:28,  2.35it/s]\u001b[A\n",
      "  3%|‚ñà‚ñç                                        | 22/652 [00:10<04:21,  2.40it/s]\u001b[A\n",
      "  4%|‚ñà‚ñç                                        | 23/652 [00:10<04:16,  2.45it/s]\u001b[A\n",
      "  4%|‚ñà‚ñå                                        | 24/652 [00:11<04:10,  2.51it/s]\u001b[A\n",
      "  4%|‚ñà‚ñå                                        | 25/652 [00:11<04:05,  2.56it/s]\u001b[A\n",
      "  4%|‚ñà‚ñã                                        | 26/652 [00:11<04:00,  2.61it/s]\u001b[A\n",
      "  4%|‚ñà‚ñã                                        | 27/652 [00:12<03:55,  2.65it/s]\u001b[A\n",
      "  4%|‚ñà‚ñä                                        | 28/652 [00:12<03:52,  2.68it/s]\u001b[A\n",
      "  4%|‚ñà‚ñä                                        | 29/652 [00:13<03:49,  2.71it/s]\u001b[A\n",
      "  5%|‚ñà‚ñâ                                        | 30/652 [00:13<03:47,  2.74it/s]\u001b[A\n",
      "  5%|‚ñà‚ñâ                                        | 31/652 [00:13<03:42,  2.79it/s]\u001b[A\n",
      "  5%|‚ñà‚ñà                                        | 32/652 [00:14<03:38,  2.84it/s]\u001b[A\n",
      "  5%|‚ñà‚ñà‚ñè                                       | 33/652 [00:14<03:35,  2.87it/s]\u001b[A\n",
      "  5%|‚ñà‚ñà‚ñè                                       | 34/652 [00:14<03:30,  2.94it/s]\u001b[A\n",
      "  5%|‚ñà‚ñà‚ñé                                       | 35/652 [00:15<03:26,  2.99it/s]\u001b[A\n",
      "  6%|‚ñà‚ñà‚ñé                                       | 36/652 [00:15<03:21,  3.06it/s]\u001b[A\n",
      "  6%|‚ñà‚ñà‚ñç                                       | 37/652 [00:15<03:15,  3.14it/s]\u001b[A\n",
      "  6%|‚ñà‚ñà‚ñç                                       | 38/652 [00:15<03:11,  3.20it/s]\u001b[A\n",
      "  6%|‚ñà‚ñà‚ñå                                       | 39/652 [00:16<03:08,  3.25it/s]\u001b[A\n",
      "  6%|‚ñà‚ñà‚ñå                                       | 40/652 [00:16<03:05,  3.31it/s]\u001b[A\n",
      "  6%|‚ñà‚ñà‚ñã                                       | 41/652 [00:16<03:00,  3.39it/s]\u001b[A\n",
      "  6%|‚ñà‚ñà‚ñã                                       | 42/652 [00:17<02:55,  3.47it/s]\u001b[A\n",
      "  7%|‚ñà‚ñà‚ñä                                       | 43/652 [00:17<02:52,  3.54it/s]\u001b[A\n",
      "  7%|‚ñà‚ñà‚ñä                                       | 44/652 [00:17<02:47,  3.63it/s]\u001b[A\n",
      "  7%|‚ñà‚ñà‚ñâ                                       | 45/652 [00:17<02:41,  3.76it/s]\u001b[A\n",
      "  7%|‚ñà‚ñà‚ñâ                                       | 46/652 [00:18<02:35,  3.89it/s]\u001b[A\n",
      "  7%|‚ñà‚ñà‚ñà                                       | 47/652 [00:18<02:30,  4.03it/s]\u001b[A\n",
      "  7%|‚ñà‚ñà‚ñà                                       | 48/652 [00:18<02:24,  4.17it/s]\u001b[A\n",
      "  8%|‚ñà‚ñà‚ñà‚ñè                                      | 49/652 [00:18<02:19,  4.34it/s]\u001b[A\n",
      "  8%|‚ñà‚ñà‚ñà‚ñè                                      | 50/652 [00:18<02:12,  4.53it/s]\u001b[A\n",
      "  8%|‚ñà‚ñà‚ñà‚ñé                                      | 51/652 [00:19<03:33,  2.81it/s]\u001b[A\n",
      "  8%|‚ñà‚ñà‚ñà‚ñé                                      | 52/652 [00:20<04:20,  2.30it/s]\u001b[A\n",
      "  8%|‚ñà‚ñà‚ñà‚ñç                                      | 53/652 [00:20<04:48,  2.08it/s]\u001b[A\n",
      "  8%|‚ñà‚ñà‚ñà‚ñç                                      | 54/652 [00:21<05:00,  1.99it/s]\u001b[A\n",
      "  8%|‚ñà‚ñà‚ñà‚ñå                                      | 55/652 [00:21<05:06,  1.95it/s]\u001b[A\n",
      "  9%|‚ñà‚ñà‚ñà‚ñå                                      | 56/652 [00:22<05:10,  1.92it/s]\u001b[A\n",
      "  9%|‚ñà‚ñà‚ñà‚ñã                                      | 57/652 [00:22<05:09,  1.93it/s]\u001b[A\n",
      "  9%|‚ñà‚ñà‚ñà‚ñã                                      | 58/652 [00:23<05:08,  1.93it/s]\u001b[A\n",
      "  9%|‚ñà‚ñà‚ñà‚ñä                                      | 59/652 [00:24<05:04,  1.95it/s]\u001b[A\n",
      "  9%|‚ñà‚ñà‚ñà‚ñä                                      | 60/652 [00:24<05:00,  1.97it/s]\u001b[A\n",
      "  9%|‚ñà‚ñà‚ñà‚ñâ                                      | 61/652 [00:25<04:58,  1.98it/s]\u001b[A\n",
      " 10%|‚ñà‚ñà‚ñà‚ñâ                                      | 62/652 [00:25<04:53,  2.01it/s]\u001b[A\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà                                      | 63/652 [00:25<04:49,  2.04it/s]\u001b[A\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà                                      | 64/652 [00:26<04:45,  2.06it/s]\u001b[A\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                     | 65/652 [00:26<04:43,  2.07it/s]\u001b[A\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñé                                     | 66/652 [00:27<04:35,  2.13it/s]\u001b[A\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñé                                     | 67/652 [00:27<04:29,  2.17it/s]\u001b[A\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                     | 68/652 [00:28<04:24,  2.21it/s]\u001b[A\n",
      " 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                     | 69/652 [00:28<04:20,  2.23it/s]\u001b[A\n",
      " 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 70/652 [00:29<04:16,  2.27it/s]\u001b[A\n",
      " 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 71/652 [00:29<04:13,  2.29it/s]\u001b[A\n",
      " 11%|‚ñà‚ñà‚ñà‚ñà‚ñã                                     | 72/652 [00:29<04:06,  2.35it/s]\u001b[A\n",
      " 11%|‚ñà‚ñà‚ñà‚ñà‚ñã                                     | 73/652 [00:30<04:02,  2.39it/s]\u001b[A\n",
      " 11%|‚ñà‚ñà‚ñà‚ñà‚ñä                                     | 74/652 [00:30<03:57,  2.43it/s]\u001b[A\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                     | 75/652 [00:31<03:53,  2.47it/s]\u001b[A\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                     | 76/652 [00:31<03:47,  2.53it/s]\u001b[A\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                     | 77/652 [00:31<03:43,  2.57it/s]\u001b[A\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñà                                     | 78/652 [00:32<03:38,  2.63it/s]\u001b[A\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñà                                     | 79/652 [00:32<03:34,  2.67it/s]\u001b[A\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                    | 80/652 [00:32<03:30,  2.71it/s]\u001b[A\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                    | 81/652 [00:33<03:26,  2.76it/s]\u001b[A\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                    | 82/652 [00:33<03:22,  2.82it/s]\u001b[A\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                    | 83/652 [00:33<03:16,  2.89it/s]\u001b[A\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 84/652 [00:34<03:12,  2.95it/s]\u001b[A\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 85/652 [00:34<03:09,  3.00it/s]\u001b[A\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 86/652 [00:34<03:06,  3.04it/s]\u001b[A\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 87/652 [00:35<03:02,  3.10it/s]\u001b[A\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                    | 88/652 [00:35<02:57,  3.18it/s]\u001b[A\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                    | 89/652 [00:35<02:52,  3.26it/s]\u001b[A\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                    | 90/652 [00:36<02:49,  3.32it/s]\u001b[A\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                    | 91/652 [00:36<02:44,  3.42it/s]\u001b[A\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                    | 92/652 [00:36<02:40,  3.50it/s]\u001b[A\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                    | 93/652 [00:36<02:37,  3.56it/s]\u001b[A\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                    | 94/652 [00:37<02:32,  3.66it/s]\u001b[A\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                    | 95/652 [00:37<02:27,  3.79it/s]\u001b[A\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 96/652 [00:37<02:21,  3.94it/s]\u001b[A\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 97/652 [00:37<02:17,  4.05it/s]\u001b[A\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 98/652 [00:38<02:09,  4.27it/s]\u001b[A\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 99/652 [00:38<02:03,  4.47it/s]\u001b[A\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 100/652 [00:38<01:58,  4.65it/s]\u001b[A\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 101/652 [00:39<03:13,  2.85it/s]\u001b[A\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 102/652 [00:39<03:58,  2.30it/s]\u001b[A\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 103/652 [00:40<04:29,  2.04it/s]\u001b[A\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 104/652 [00:40<04:50,  1.89it/s]\u001b[A\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 105/652 [00:41<04:57,  1.84it/s]\u001b[A\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 106/652 [00:42<05:02,  1.81it/s]\u001b[A\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 107/652 [00:42<05:02,  1.80it/s]\u001b[A\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 108/652 [00:43<05:01,  1.80it/s]\u001b[A\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 109/652 [00:43<05:00,  1.81it/s]\u001b[A\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 110/652 [00:44<04:56,  1.83it/s]\u001b[A\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 111/652 [00:44<04:51,  1.86it/s]\u001b[A\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 112/652 [00:45<04:45,  1.89it/s]\u001b[A\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 113/652 [00:45<04:38,  1.93it/s]\u001b[A\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 114/652 [00:46<04:31,  1.98it/s]\u001b[A\n",
      " 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 115/652 [00:46<04:26,  2.02it/s]\u001b[A\n",
      " 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 116/652 [00:47<04:21,  2.05it/s]\u001b[A\n",
      " 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 117/652 [00:47<04:13,  2.11it/s]\u001b[A\n",
      " 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 118/652 [00:48<04:07,  2.16it/s]\u001b[A\n",
      " 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 119/652 [00:48<04:01,  2.21it/s]\u001b[A\n",
      " 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 120/652 [00:48<03:56,  2.24it/s]\u001b[A\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 121/652 [00:49<03:53,  2.27it/s]\u001b[A\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 122/652 [00:49<03:46,  2.33it/s]\u001b[A\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 123/652 [00:50<03:41,  2.39it/s]\u001b[A\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 124/652 [00:50<03:36,  2.44it/s]\u001b[A\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 125/652 [00:50<03:30,  2.50it/s]\u001b[A\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 126/652 [00:51<03:25,  2.56it/s]\u001b[A\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 127/652 [00:51<03:20,  2.61it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 128/652 [00:52<03:17,  2.66it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 129/652 [00:52<03:14,  2.69it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 130/652 [00:52<03:10,  2.75it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 131/652 [00:53<03:05,  2.80it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 132/652 [00:53<03:02,  2.85it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 133/652 [00:53<02:57,  2.93it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 134/652 [00:54<02:53,  2.98it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 135/652 [00:54<02:48,  3.06it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 136/652 [00:54<02:44,  3.13it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 137/652 [00:55<02:41,  3.18it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 138/652 [00:55<02:38,  3.24it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 139/652 [00:55<02:34,  3.31it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 140/652 [00:55<02:30,  3.41it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 141/652 [00:56<02:26,  3.49it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 142/652 [00:56<02:22,  3.59it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 143/652 [00:56<02:18,  3.67it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 144/652 [00:56<02:13,  3.81it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 145/652 [00:57<02:08,  3.96it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 146/652 [00:57<02:01,  4.17it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 147/652 [00:57<01:56,  4.35it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 148/652 [00:57<01:51,  4.53it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 149/652 [00:57<01:47,  4.67it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 150/652 [00:58<01:42,  4.90it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 151/652 [00:58<02:54,  2.88it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 152/652 [00:59<03:35,  2.32it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 153/652 [01:00<03:56,  2.11it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 154/652 [01:00<04:10,  1.98it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 155/652 [01:01<04:18,  1.92it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 157/652 [01:02<04:22,  1.89it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 158/652 [01:02<04:20,  1.89it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 159/652 [01:03<04:18,  1.91it/s]\u001b[A\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 160/652 [01:03<04:14,  1.93it/s]\u001b[A\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 161/652 [01:04<04:12,  1.94it/s]\u001b[A\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 162/652 [01:04<04:07,  1.98it/s]\u001b[A\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 163/652 [01:05<04:03,  2.01it/s]\u001b[A\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 164/652 [01:05<03:59,  2.04it/s]\u001b[A\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 165/652 [01:06<03:56,  2.06it/s]\u001b[A\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 166/652 [01:06<03:52,  2.09it/s]\u001b[A\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 167/652 [01:07<03:46,  2.14it/s]\u001b[A\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 168/652 [01:07<03:41,  2.18it/s]\u001b[A\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 169/652 [01:07<03:37,  2.22it/s]\u001b[A\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 170/652 [01:08<03:34,  2.25it/s]\u001b[A\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 171/652 [01:08<03:30,  2.28it/s]\u001b[A\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 172/652 [01:09<03:28,  2.30it/s]\u001b[A\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 173/652 [01:09<03:23,  2.35it/s]\u001b[A\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 174/652 [01:10<03:18,  2.40it/s]\u001b[A\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 175/652 [01:10<03:15,  2.44it/s]\u001b[A\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 176/652 [01:10<03:09,  2.52it/s]\u001b[A\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 177/652 [01:11<03:05,  2.57it/s]\u001b[A\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 178/652 [01:11<03:01,  2.61it/s]\u001b[A\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 179/652 [01:11<02:57,  2.67it/s]\u001b[A\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 180/652 [01:12<02:52,  2.73it/s]\u001b[A\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 181/652 [01:12<02:49,  2.78it/s]\u001b[A\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 182/652 [01:12<02:46,  2.82it/s]\u001b[A\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 183/652 [01:13<02:43,  2.86it/s]\u001b[A\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 184/652 [01:13<02:39,  2.93it/s]\u001b[A\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 185/652 [01:13<02:36,  2.99it/s]\u001b[A\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 186/652 [01:14<02:33,  3.03it/s]\u001b[A\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 187/652 [01:14<02:29,  3.11it/s]\u001b[A\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 188/652 [01:14<02:26,  3.17it/s]\u001b[A\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 189/652 [01:15<02:23,  3.22it/s]\u001b[A\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 190/652 [01:15<02:20,  3.28it/s]\u001b[A\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 191/652 [01:15<02:18,  3.34it/s]\u001b[A\n",
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 192/652 [01:15<02:14,  3.43it/s]\u001b[A\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 193/652 [01:16<02:10,  3.52it/s]\u001b[A\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 194/652 [01:16<02:04,  3.66it/s]\u001b[A\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 195/652 [01:16<02:00,  3.80it/s]\u001b[A\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 196/652 [01:16<01:55,  3.96it/s]\u001b[A\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 197/652 [01:17<01:49,  4.17it/s]\u001b[A\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 198/652 [01:17<01:44,  4.35it/s]\u001b[A\n",
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 199/652 [01:17<01:39,  4.53it/s]\u001b[A\n",
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 200/652 [01:17<01:34,  4.77it/s]\u001b[A\n",
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 201/652 [01:18<02:47,  2.70it/s]\u001b[A\n",
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 202/652 [01:19<03:22,  2.22it/s]\u001b[A\n",
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 203/652 [01:19<03:45,  1.99it/s]\u001b[A\n",
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 204/652 [01:20<03:57,  1.89it/s]\u001b[A\n",
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 205/652 [01:20<04:05,  1.82it/s]\u001b[A\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 206/652 [01:21<04:08,  1.80it/s]\u001b[A\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 207/652 [01:22<04:08,  1.79it/s]\u001b[A\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 208/652 [01:22<04:06,  1.80it/s]\u001b[A\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 209/652 [01:23<04:05,  1.80it/s]\u001b[A\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 210/652 [01:23<04:00,  1.84it/s]\u001b[A\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 211/652 [01:24<03:55,  1.87it/s]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 212/652 [01:24<03:49,  1.91it/s]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 213/652 [01:25<03:43,  1.96it/s]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 214/652 [01:25<03:39,  2.00it/s]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 215/652 [01:26<03:35,  2.03it/s]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 216/652 [01:26<03:30,  2.07it/s]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 217/652 [01:27<03:25,  2.12it/s]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 218/652 [01:27<03:20,  2.17it/s]\u001b[A\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 219/652 [01:27<03:16,  2.21it/s]\u001b[A\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 220/652 [01:28<03:12,  2.24it/s]\u001b[A\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 221/652 [01:28<03:09,  2.27it/s]\u001b[A\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 222/652 [01:29<03:05,  2.32it/s]\u001b[A\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 223/652 [01:29<03:01,  2.36it/s]\u001b[A\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 224/652 [01:29<02:58,  2.40it/s]\u001b[A\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 225/652 [01:30<02:56,  2.43it/s]\u001b[A\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 226/652 [01:30<02:52,  2.47it/s]\u001b[A\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 227/652 [01:31<02:47,  2.54it/s]\u001b[A\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 228/652 [01:31<02:43,  2.59it/s]\u001b[A\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 229/652 [01:31<02:39,  2.64it/s]\u001b[A\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 230/652 [01:32<02:37,  2.69it/s]\u001b[A\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 231/652 [01:32<02:34,  2.72it/s]\u001b[A\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 232/652 [01:32<02:30,  2.79it/s]\u001b[A\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 233/652 [01:33<02:26,  2.87it/s]\u001b[A\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 234/652 [01:33<02:22,  2.94it/s]\u001b[A\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 235/652 [01:33<02:17,  3.03it/s]\u001b[A\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 236/652 [01:34<02:13,  3.12it/s]\u001b[A\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 237/652 [01:34<02:09,  3.19it/s]\u001b[A\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 238/652 [01:34<02:06,  3.27it/s]\u001b[A\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 239/652 [01:35<02:04,  3.32it/s]\u001b[A\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 240/652 [01:35<02:00,  3.41it/s]\u001b[A\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 241/652 [01:35<01:58,  3.45it/s]\u001b[A\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 242/652 [01:35<01:54,  3.57it/s]\u001b[A\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 243/652 [01:36<01:51,  3.66it/s]\u001b[A\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 244/652 [01:36<01:48,  3.77it/s]\u001b[A\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 245/652 [01:36<01:45,  3.86it/s]\u001b[A\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 246/652 [01:36<01:40,  4.02it/s]\u001b[A\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 247/652 [01:37<01:36,  4.22it/s]\u001b[A\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 248/652 [01:37<01:32,  4.38it/s]\u001b[A\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 249/652 [01:37<01:28,  4.53it/s]\u001b[A\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 250/652 [01:37<01:24,  4.77it/s]\u001b[A\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 251/652 [01:38<02:20,  2.86it/s]\u001b[A\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 252/652 [01:38<02:54,  2.29it/s]\u001b[A\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 253/652 [01:39<03:13,  2.06it/s]\u001b[A\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 254/652 [01:40<03:26,  1.93it/s]\u001b[A\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 255/652 [01:40<03:33,  1.86it/s]\u001b[A\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 256/652 [01:41<03:37,  1.82it/s]\u001b[A\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 257/652 [01:41<03:38,  1.81it/s]\u001b[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 258/652 [01:42<03:38,  1.81it/s]\u001b[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 259/652 [01:42<03:35,  1.82it/s]\u001b[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 260/652 [01:43<03:32,  1.85it/s]\u001b[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 261/652 [01:44<03:29,  1.87it/s]\u001b[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 262/652 [01:44<03:26,  1.89it/s]\u001b[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 263/652 [01:45<03:22,  1.92it/s]\u001b[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 264/652 [01:45<03:17,  1.97it/s]\u001b[A\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 265/652 [01:45<03:13,  2.00it/s]\u001b[A\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 266/652 [01:46<03:08,  2.05it/s]\u001b[A\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 267/652 [01:46<03:02,  2.11it/s]\u001b[A\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 268/652 [01:47<02:58,  2.16it/s]\u001b[A\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 269/652 [01:47<02:54,  2.20it/s]\u001b[A\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 270/652 [01:48<02:51,  2.23it/s]\u001b[A\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 271/652 [01:48<02:48,  2.26it/s]\u001b[A\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 272/652 [01:49<02:45,  2.29it/s]\u001b[A\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 273/652 [01:49<02:42,  2.33it/s]\u001b[A\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 274/652 [01:49<02:39,  2.37it/s]\u001b[A\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 275/652 [01:50<02:36,  2.41it/s]\u001b[A\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 276/652 [01:50<02:33,  2.44it/s]\u001b[A\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 277/652 [01:51<02:31,  2.47it/s]\u001b[A\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 278/652 [01:51<02:29,  2.50it/s]\u001b[A\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 279/652 [01:51<02:26,  2.55it/s]\u001b[A\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 280/652 [01:52<02:23,  2.59it/s]\u001b[A\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 281/652 [01:52<02:20,  2.64it/s]\u001b[A\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 282/652 [01:52<02:18,  2.68it/s]\u001b[A\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 283/652 [01:53<02:16,  2.71it/s]\u001b[A\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 284/652 [01:53<02:12,  2.77it/s]\u001b[A\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 285/652 [01:53<02:09,  2.82it/s]\u001b[A\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 286/652 [01:54<02:05,  2.91it/s]\u001b[A\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 287/652 [01:54<02:01,  3.02it/s]\u001b[A\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 288/652 [01:54<01:57,  3.11it/s]\u001b[A\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 289/652 [01:55<01:54,  3.18it/s]\u001b[A\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 290/652 [01:55<01:49,  3.30it/s]\u001b[A\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 291/652 [01:55<01:46,  3.40it/s]\u001b[A\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 292/652 [01:55<01:43,  3.48it/s]\u001b[A\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 293/652 [01:56<01:40,  3.58it/s]\u001b[A\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 294/652 [01:56<01:37,  3.68it/s]\u001b[A\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 295/652 [01:56<01:33,  3.81it/s]\u001b[A\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 296/652 [01:56<01:29,  3.99it/s]\u001b[A\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 297/652 [01:57<01:24,  4.21it/s]\u001b[A\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 298/652 [01:57<01:20,  4.39it/s]\u001b[A\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 299/652 [01:57<01:17,  4.55it/s]\u001b[A\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 300/652 [01:57<01:13,  4.78it/s]\u001b[A\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 301/652 [01:58<02:03,  2.84it/s]\u001b[A\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 302/652 [01:59<02:32,  2.30it/s]\u001b[A\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 303/652 [01:59<02:48,  2.07it/s]\u001b[A\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 304/652 [02:00<02:55,  1.98it/s]\u001b[A\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 305/652 [02:00<03:00,  1.92it/s]\u001b[A\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 306/652 [02:01<03:02,  1.90it/s]\u001b[A\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 307/652 [02:01<03:02,  1.89it/s]\u001b[A\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 308/652 [02:02<03:01,  1.89it/s]\u001b[A\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 309/652 [02:02<03:00,  1.90it/s]\u001b[A\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 310/652 [02:03<02:58,  1.91it/s]\u001b[A\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 311/652 [02:03<02:55,  1.94it/s]\u001b[A\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 312/652 [02:04<02:53,  1.96it/s]\u001b[A\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 313/652 [02:04<02:49,  2.00it/s]\u001b[A\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 314/652 [02:05<02:46,  2.03it/s]\u001b[A\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 315/652 [02:05<02:44,  2.05it/s]\u001b[A\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 316/652 [02:06<02:41,  2.08it/s]\u001b[A\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 317/652 [02:06<02:39,  2.10it/s]\u001b[A\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 318/652 [02:07<02:34,  2.16it/s]\u001b[A\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 319/652 [02:07<02:31,  2.20it/s]\u001b[A\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 320/652 [02:08<02:27,  2.24it/s]\u001b[A\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 321/652 [02:08<02:26,  2.27it/s]\u001b[A\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 322/652 [02:08<02:22,  2.32it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 323/652 [02:09<02:18,  2.37it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 324/652 [02:09<02:15,  2.42it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 325/652 [02:10<02:13,  2.45it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 326/652 [02:10<02:09,  2.52it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 327/652 [02:10<02:06,  2.57it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 328/652 [02:11<02:03,  2.63it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 329/652 [02:11<02:00,  2.68it/s]\u001b[A\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 330/652 [02:11<01:58,  2.71it/s]\u001b[A\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 331/652 [02:12<01:56,  2.76it/s]\u001b[A\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 332/652 [02:12<01:53,  2.81it/s]\u001b[A\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 333/652 [02:12<01:51,  2.85it/s]\u001b[A\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 334/652 [02:13<01:48,  2.92it/s]\u001b[A\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 335/652 [02:13<01:46,  2.97it/s]\u001b[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 336/652 [02:13<01:44,  3.02it/s]\u001b[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 337/652 [02:14<01:41,  3.09it/s]\u001b[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 338/652 [02:14<01:39,  3.15it/s]\u001b[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 339/652 [02:14<01:36,  3.23it/s]\u001b[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 340/652 [02:15<01:34,  3.29it/s]\u001b[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 341/652 [02:15<01:33,  3.34it/s]\u001b[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 342/652 [02:15<01:30,  3.44it/s]\u001b[A\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 343/652 [02:15<01:27,  3.55it/s]\u001b[A\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 344/652 [02:16<01:24,  3.64it/s]\u001b[A\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 345/652 [02:16<01:22,  3.73it/s]\u001b[A\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 346/652 [02:16<01:19,  3.83it/s]\u001b[A\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 347/652 [02:16<01:16,  4.00it/s]\u001b[A\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 348/652 [02:17<01:11,  4.25it/s]\u001b[A\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 349/652 [02:17<01:06,  4.55it/s]\u001b[A\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 350/652 [02:17<01:00,  4.97it/s]\u001b[A\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 351/652 [02:18<01:39,  3.01it/s]\u001b[A\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 352/652 [02:18<02:03,  2.43it/s]\u001b[A\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 353/652 [02:19<02:20,  2.13it/s]\u001b[A\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 354/652 [02:19<02:29,  2.00it/s]\u001b[A\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 355/652 [02:20<02:33,  1.93it/s]\u001b[A\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 356/652 [02:20<02:36,  1.89it/s]\u001b[A\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 357/652 [02:21<02:37,  1.88it/s]\u001b[A\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 358/652 [02:22<02:35,  1.89it/s]\u001b[A\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 359/652 [02:22<02:34,  1.90it/s]\u001b[A\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 360/652 [02:23<02:31,  1.93it/s]\u001b[A\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 361/652 [02:23<02:27,  1.97it/s]\u001b[A\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 362/652 [02:24<02:24,  2.00it/s]\u001b[A\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 363/652 [02:24<02:21,  2.04it/s]\u001b[A\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 364/652 [02:24<02:19,  2.06it/s]\u001b[A\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 365/652 [02:25<02:17,  2.09it/s]\u001b[A\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 366/652 [02:25<02:13,  2.14it/s]\u001b[A\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 367/652 [02:26<02:10,  2.19it/s]\u001b[A\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 368/652 [02:26<02:07,  2.22it/s]\u001b[A\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 369/652 [02:27<02:06,  2.24it/s]\u001b[A\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 370/652 [02:27<02:03,  2.27it/s]\u001b[A\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 371/652 [02:27<02:00,  2.33it/s]\u001b[A\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 372/652 [02:28<01:57,  2.37it/s]\u001b[A\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 373/652 [02:28<01:55,  2.41it/s]\u001b[A\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 374/652 [02:29<01:53,  2.45it/s]\u001b[A\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 375/652 [02:29<01:51,  2.48it/s]\u001b[A\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 376/652 [02:29<01:48,  2.54it/s]\u001b[A\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 377/652 [02:30<01:45,  2.60it/s]\u001b[A\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 378/652 [02:30<01:43,  2.65it/s]\u001b[A\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 379/652 [02:31<01:41,  2.68it/s]\u001b[A\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 380/652 [02:31<01:40,  2.71it/s]\u001b[A\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 381/652 [02:31<01:39,  2.73it/s]\u001b[A\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 382/652 [02:32<01:38,  2.75it/s]\u001b[A\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 383/652 [02:32<01:36,  2.79it/s]\u001b[A\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 384/652 [02:32<01:34,  2.84it/s]\u001b[A\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 385/652 [02:33<01:33,  2.85it/s]\u001b[A\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 386/652 [02:33<01:30,  2.93it/s]\u001b[A\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 387/652 [02:33<01:28,  2.99it/s]\u001b[A\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 388/652 [02:34<01:25,  3.08it/s]\u001b[A\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 389/652 [02:34<01:23,  3.15it/s]\u001b[A\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 390/652 [02:34<01:24,  3.11it/s]\u001b[A\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 391/652 [02:35<01:22,  3.18it/s]\u001b[A\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 392/652 [02:35<01:19,  3.29it/s]\u001b[A\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 393/652 [02:35<01:15,  3.43it/s]\u001b[A\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 394/652 [02:35<01:12,  3.55it/s]\u001b[A\n",
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 395/652 [02:36<01:10,  3.66it/s]\u001b[A\n",
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 396/652 [02:36<01:07,  3.80it/s]\u001b[A\n",
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 397/652 [02:36<01:04,  3.97it/s]\u001b[A\n",
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 398/652 [02:36<01:01,  4.11it/s]\u001b[A\n",
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 399/652 [02:36<00:58,  4.31it/s]\u001b[A\n",
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 400/652 [02:37<00:54,  4.62it/s]\u001b[A\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 401/652 [02:37<01:29,  2.79it/s]\u001b[A\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 402/652 [02:38<01:49,  2.29it/s]\u001b[A\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 403/652 [02:39<01:59,  2.09it/s]\u001b[A\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 404/652 [02:39<02:03,  2.00it/s]\u001b[A\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 405/652 [02:40<02:06,  1.95it/s]\u001b[A\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 406/652 [02:40<02:08,  1.92it/s]\u001b[A\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 407/652 [02:41<02:08,  1.90it/s]\u001b[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 408/652 [02:41<02:07,  1.91it/s]\u001b[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 409/652 [02:42<02:05,  1.93it/s]\u001b[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 410/652 [02:42<02:03,  1.95it/s]\u001b[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 411/652 [02:43<02:02,  1.97it/s]\u001b[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 412/652 [02:43<01:59,  2.00it/s]\u001b[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 413/652 [02:44<01:57,  2.04it/s]\u001b[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 414/652 [02:44<01:55,  2.07it/s]\u001b[A\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 415/652 [02:45<01:53,  2.10it/s]\u001b[A\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 416/652 [02:45<01:49,  2.15it/s]\u001b[A\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 417/652 [02:45<01:47,  2.18it/s]\u001b[A\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 418/652 [02:46<01:45,  2.22it/s]\u001b[A\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 419/652 [02:46<01:43,  2.26it/s]\u001b[A\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 420/652 [02:47<01:41,  2.29it/s]\u001b[A\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 421/652 [02:47<01:38,  2.34it/s]\u001b[A\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 422/652 [02:48<01:36,  2.37it/s]\u001b[A\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 423/652 [02:48<01:34,  2.41it/s]\u001b[A\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 424/652 [02:48<01:33,  2.45it/s]\u001b[A\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 425/652 [02:49<01:30,  2.51it/s]\u001b[A\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 426/652 [02:49<01:28,  2.56it/s]\u001b[A\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 427/652 [02:49<01:26,  2.60it/s]\u001b[A\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 428/652 [02:50<01:25,  2.63it/s]\u001b[A\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 429/652 [02:50<01:23,  2.66it/s]\u001b[A\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 430/652 [02:51<01:22,  2.70it/s]\u001b[A\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 431/652 [02:51<01:20,  2.73it/s]\u001b[A\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 432/652 [02:51<01:19,  2.78it/s]\u001b[A\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 433/652 [02:52<01:17,  2.82it/s]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 434/652 [02:52<01:16,  2.85it/s]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 435/652 [02:52<01:14,  2.92it/s]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 436/652 [02:53<01:11,  3.03it/s]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 437/652 [02:53<01:09,  3.11it/s]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 438/652 [02:53<01:07,  3.18it/s]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 439/652 [02:53<01:05,  3.25it/s]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 440/652 [02:54<01:04,  3.30it/s]\u001b[A\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 441/652 [02:54<01:02,  3.40it/s]\u001b[A\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 442/652 [02:54<01:00,  3.49it/s]\u001b[A\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 443/652 [02:55<00:58,  3.56it/s]\u001b[A\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 444/652 [02:55<00:57,  3.64it/s]\u001b[A\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 445/652 [02:55<00:55,  3.72it/s]\u001b[A\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 446/652 [02:55<00:53,  3.84it/s]\u001b[A\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 447/652 [02:56<00:52,  3.94it/s]\u001b[A\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 448/652 [02:56<00:49,  4.16it/s]\u001b[A\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 449/652 [02:56<00:46,  4.35it/s]\u001b[A\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 450/652 [02:56<00:43,  4.64it/s]\u001b[A\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 451/652 [02:57<01:16,  2.64it/s]\u001b[A\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 452/652 [02:58<01:31,  2.19it/s]\u001b[A\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 453/652 [02:58<01:39,  2.01it/s]\u001b[A\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 454/652 [02:59<01:44,  1.90it/s]\u001b[A\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 455/652 [02:59<01:46,  1.84it/s]\u001b[A\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 456/652 [03:00<01:47,  1.83it/s]\u001b[A\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 457/652 [03:00<01:47,  1.82it/s]\u001b[A\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 458/652 [03:01<01:46,  1.82it/s]\u001b[A\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 459/652 [03:02<01:45,  1.83it/s]\u001b[A\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 460/652 [03:02<01:43,  1.86it/s]\u001b[A\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 461/652 [03:03<01:41,  1.87it/s]\u001b[A\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 462/652 [03:03<01:40,  1.89it/s]\u001b[A\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 463/652 [03:04<01:38,  1.92it/s]\u001b[A\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 464/652 [03:04<01:36,  1.95it/s]\u001b[A\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 465/652 [03:05<01:34,  1.98it/s]\u001b[A\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 466/652 [03:05<01:32,  2.02it/s]\u001b[A\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 467/652 [03:06<01:29,  2.06it/s]\u001b[A\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 468/652 [03:06<01:28,  2.09it/s]\u001b[A\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 469/652 [03:06<01:26,  2.11it/s]\u001b[A\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 470/652 [03:07<01:24,  2.16it/s]\u001b[A\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 471/652 [03:07<01:22,  2.20it/s]\u001b[A\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 472/652 [03:08<01:20,  2.23it/s]\u001b[A\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 473/652 [03:08<01:19,  2.26it/s]\u001b[A\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 474/652 [03:09<01:17,  2.29it/s]\u001b[A\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 475/652 [03:09<01:15,  2.34it/s]\u001b[A\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 476/652 [03:09<01:13,  2.39it/s]\u001b[A\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 477/652 [03:10<01:12,  2.42it/s]\u001b[A\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 478/652 [03:10<01:10,  2.45it/s]\u001b[A\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 479/652 [03:11<01:09,  2.48it/s]\u001b[A\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 480/652 [03:11<01:08,  2.50it/s]\u001b[A\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 481/652 [03:11<01:07,  2.55it/s]\u001b[A\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 482/652 [03:12<01:05,  2.59it/s]\u001b[A\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 483/652 [03:12<01:04,  2.63it/s]\u001b[A\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 484/652 [03:12<01:03,  2.67it/s]\u001b[A\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 485/652 [03:13<01:01,  2.70it/s]\u001b[A\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 486/652 [03:13<01:00,  2.76it/s]\u001b[A\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 487/652 [03:13<00:57,  2.86it/s]\u001b[A\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 488/652 [03:14<00:55,  2.98it/s]\u001b[A\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 490/652 [03:14<00:51,  3.17it/s]\u001b[A\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 491/652 [03:15<00:49,  3.25it/s]\u001b[A\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 492/652 [03:15<00:47,  3.38it/s]\u001b[A\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 493/652 [03:15<00:45,  3.49it/s]\u001b[A\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 494/652 [03:15<00:43,  3.59it/s]\u001b[A\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 495/652 [03:16<00:42,  3.72it/s]\u001b[A\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 496/652 [03:16<00:39,  3.90it/s]\u001b[A\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 497/652 [03:16<00:38,  4.05it/s]\u001b[A\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 498/652 [03:16<00:36,  4.27it/s]\u001b[A\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 499/652 [03:17<00:34,  4.47it/s]\u001b[A\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 500/652 [03:17<00:31,  4.77it/s]\u001b[A\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 501/652 [03:17<00:53,  2.81it/s]\u001b[A\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 502/652 [03:18<01:05,  2.28it/s]\u001b[A\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 503/652 [03:19<01:12,  2.06it/s]\u001b[A\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 504/652 [03:19<01:15,  1.96it/s]\u001b[A\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 505/652 [03:20<01:17,  1.91it/s]\u001b[A\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 506/652 [03:20<01:17,  1.88it/s]\u001b[A\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 507/652 [03:21<01:17,  1.86it/s]\u001b[A\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 508/652 [03:21<01:16,  1.88it/s]\u001b[A\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 509/652 [03:22<01:15,  1.89it/s]\u001b[A\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 510/652 [03:22<01:14,  1.92it/s]\u001b[A\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 511/652 [03:23<01:12,  1.94it/s]\u001b[A\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 512/652 [03:23<01:10,  1.98it/s]\u001b[A\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 513/652 [03:24<01:09,  2.00it/s]\u001b[A\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 514/652 [03:24<01:08,  2.03it/s]\u001b[A\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 515/652 [03:25<01:06,  2.04it/s]\u001b[A\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 516/652 [03:25<01:05,  2.07it/s]\u001b[A\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 517/652 [03:26<01:04,  2.09it/s]\u001b[A\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 518/652 [03:26<01:02,  2.14it/s]\u001b[A\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 519/652 [03:27<01:01,  2.18it/s]\u001b[A\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 520/652 [03:27<00:59,  2.22it/s]\u001b[A\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 521/652 [03:28<00:58,  2.25it/s]\u001b[A\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 522/652 [03:28<00:57,  2.28it/s]\u001b[A\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 523/652 [03:28<00:55,  2.33it/s]\u001b[A\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 524/652 [03:29<00:53,  2.38it/s]\u001b[A\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 525/652 [03:29<00:52,  2.42it/s]\u001b[A\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 526/652 [03:30<00:51,  2.45it/s]\u001b[A\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 527/652 [03:30<00:49,  2.52it/s]\u001b[A\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 528/652 [03:30<00:48,  2.57it/s]\u001b[A\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 529/652 [03:31<00:47,  2.61it/s]\u001b[A\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 530/652 [03:31<00:45,  2.66it/s]\u001b[A\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 531/652 [03:31<00:44,  2.73it/s]\u001b[A\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 532/652 [03:32<00:43,  2.79it/s]\u001b[A\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 533/652 [03:32<00:41,  2.87it/s]\u001b[A\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 534/652 [03:32<00:40,  2.93it/s]\u001b[A\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 535/652 [03:33<00:39,  2.99it/s]\u001b[A\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 536/652 [03:33<00:37,  3.08it/s]\u001b[A\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 537/652 [03:33<00:36,  3.15it/s]\u001b[A\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 538/652 [03:34<00:35,  3.21it/s]\u001b[A\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 539/652 [03:34<00:34,  3.27it/s]\u001b[A\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 540/652 [03:34<00:33,  3.33it/s]\u001b[A\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 541/652 [03:34<00:32,  3.43it/s]\u001b[A\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 542/652 [03:35<00:31,  3.54it/s]\u001b[A\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 543/652 [03:35<00:29,  3.64it/s]\u001b[A\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 544/652 [03:35<00:28,  3.75it/s]\u001b[A\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 545/652 [03:35<00:27,  3.84it/s]\u001b[A\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 546/652 [03:36<00:26,  4.01it/s]\u001b[A\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 547/652 [03:36<00:24,  4.22it/s]\u001b[A\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 548/652 [03:36<00:23,  4.41it/s]\u001b[A\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 549/652 [03:36<00:22,  4.58it/s]\u001b[A\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 550/652 [03:36<00:21,  4.80it/s]\u001b[A\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 551/652 [03:37<00:38,  2.61it/s]\u001b[A\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 552/652 [03:38<00:45,  2.19it/s]\u001b[A\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 553/652 [03:38<00:49,  2.01it/s]\u001b[A\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 554/652 [03:39<00:51,  1.90it/s]\u001b[A\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 555/652 [03:40<00:52,  1.84it/s]\u001b[A\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 556/652 [03:40<00:52,  1.81it/s]\u001b[A\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 557/652 [03:41<00:52,  1.81it/s]\u001b[A\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 558/652 [03:41<00:51,  1.82it/s]\u001b[A\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 559/652 [03:42<00:50,  1.83it/s]\u001b[A\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 560/652 [03:42<00:49,  1.85it/s]\u001b[A\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 561/652 [03:43<00:48,  1.86it/s]\u001b[A\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 562/652 [03:43<00:47,  1.88it/s]\u001b[A\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 563/652 [03:44<00:46,  1.91it/s]\u001b[A\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 564/652 [03:44<00:45,  1.93it/s]\u001b[A\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 565/652 [03:45<00:44,  1.96it/s]\u001b[A\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 566/652 [03:45<00:42,  2.00it/s]\u001b[A\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 567/652 [03:46<00:41,  2.03it/s]\u001b[A\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 568/652 [03:46<00:40,  2.07it/s]\u001b[A\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 569/652 [03:47<00:39,  2.13it/s]\u001b[A\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 570/652 [03:47<00:37,  2.17it/s]\u001b[A\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 571/652 [03:48<00:36,  2.20it/s]\u001b[A\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 572/652 [03:48<00:35,  2.23it/s]\u001b[A\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 573/652 [03:49<00:34,  2.26it/s]\u001b[A\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 574/652 [03:49<00:34,  2.28it/s]\u001b[A\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 575/652 [03:49<00:32,  2.33it/s]\u001b[A\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 576/652 [03:50<00:31,  2.38it/s]\u001b[A\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 577/652 [03:50<00:30,  2.42it/s]\u001b[A\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 578/652 [03:51<00:29,  2.49it/s]\u001b[A\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 579/652 [03:51<00:28,  2.55it/s]\u001b[A\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 580/652 [03:51<00:27,  2.59it/s]\u001b[A\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 581/652 [03:52<00:26,  2.64it/s]\u001b[A\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 582/652 [03:52<00:26,  2.68it/s]\u001b[A\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 583/652 [03:52<00:25,  2.71it/s]\u001b[A\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 584/652 [03:53<00:24,  2.77it/s]\u001b[A\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 585/652 [03:53<00:23,  2.83it/s]\u001b[A\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 586/652 [03:53<00:22,  2.90it/s]\u001b[A\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 587/652 [03:54<00:21,  2.96it/s]\u001b[A\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 588/652 [03:54<00:20,  3.06it/s]\u001b[A\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 589/652 [03:54<00:19,  3.15it/s]\u001b[A\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 590/652 [03:55<00:19,  3.24it/s]\u001b[A\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 591/652 [03:55<00:18,  3.35it/s]\u001b[A\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 592/652 [03:55<00:17,  3.45it/s]\u001b[A\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 593/652 [03:55<00:16,  3.54it/s]\u001b[A\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 594/652 [03:56<00:15,  3.64it/s]\u001b[A\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 595/652 [03:56<00:15,  3.78it/s]\u001b[A\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 596/652 [03:56<00:14,  3.94it/s]\u001b[A\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 597/652 [03:56<00:13,  4.09it/s]\u001b[A\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 598/652 [03:57<00:12,  4.30it/s]\u001b[A\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 599/652 [03:57<00:11,  4.51it/s]\u001b[A\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 600/652 [03:57<00:10,  4.77it/s]\u001b[A\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 601/652 [03:58<00:18,  2.77it/s]\u001b[A\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 602/652 [03:58<00:22,  2.27it/s]\u001b[A\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 603/652 [03:59<00:23,  2.06it/s]\u001b[A\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 604/652 [03:59<00:24,  1.96it/s]\u001b[A\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 605/652 [04:00<00:24,  1.91it/s]\u001b[A\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 606/652 [04:01<00:24,  1.88it/s]\u001b[A\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 607/652 [04:01<00:24,  1.87it/s]\u001b[A\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 608/652 [04:02<00:23,  1.86it/s]\u001b[A\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 609/652 [04:02<00:23,  1.86it/s]\u001b[A\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 610/652 [04:03<00:22,  1.88it/s]\u001b[A\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 611/652 [04:03<00:21,  1.90it/s]\u001b[A\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 612/652 [04:04<00:20,  1.93it/s]\u001b[A\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 613/652 [04:04<00:20,  1.95it/s]\u001b[A\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 614/652 [04:05<00:19,  1.96it/s]\u001b[A\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 615/652 [04:05<00:18,  1.99it/s]\u001b[A\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 616/652 [04:06<00:17,  2.03it/s]\u001b[A\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 617/652 [04:06<00:16,  2.06it/s]\u001b[A\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 618/652 [04:07<00:16,  2.12it/s]\u001b[A\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 619/652 [04:07<00:15,  2.17it/s]\u001b[A\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 620/652 [04:07<00:14,  2.21it/s]\u001b[A\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 621/652 [04:08<00:13,  2.25it/s]\u001b[A\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 622/652 [04:08<00:13,  2.28it/s]\u001b[A\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 623/652 [04:09<00:12,  2.33it/s]\u001b[A\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 624/652 [04:09<00:11,  2.37it/s]\u001b[A\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 625/652 [04:09<00:11,  2.39it/s]\u001b[A\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 626/652 [04:10<00:10,  2.43it/s]\u001b[A\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 627/652 [04:10<00:10,  2.46it/s]\u001b[A\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 628/652 [04:11<00:09,  2.52it/s]\u001b[A\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 629/652 [04:11<00:08,  2.57it/s]\u001b[A\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 630/652 [04:11<00:08,  2.62it/s]\u001b[A\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 631/652 [04:12<00:07,  2.70it/s]\u001b[A\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 632/652 [04:12<00:07,  2.76it/s]\u001b[A\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 633/652 [04:12<00:06,  2.80it/s]\u001b[A\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 634/652 [04:13<00:06,  2.87it/s]\u001b[A\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 635/652 [04:13<00:05,  2.93it/s]\u001b[A\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 636/652 [04:13<00:05,  2.99it/s]\u001b[A\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 637/652 [04:14<00:04,  3.08it/s]\u001b[A\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 638/652 [04:14<00:04,  3.18it/s]\u001b[A\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 639/652 [04:14<00:03,  3.29it/s]\u001b[A\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 640/652 [04:15<00:03,  3.40it/s]\u001b[A\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 641/652 [04:15<00:03,  3.49it/s]\u001b[A\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 642/652 [04:15<00:02,  3.59it/s]\u001b[A\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 643/652 [04:15<00:02,  3.74it/s]\u001b[A\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 644/652 [04:16<00:02,  3.87it/s]\u001b[A\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 645/652 [04:16<00:01,  4.01it/s]\u001b[A\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 646/652 [04:16<00:01,  4.12it/s]\u001b[A\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 647/652 [04:16<00:01,  4.31it/s]\u001b[A\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 648/652 [04:16<00:00,  4.50it/s]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 649/652 [04:17<00:00,  4.74it/s]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 650/652 [04:17<00:00,  5.00it/s]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 651/652 [04:17<00:00,  3.44it/s]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 652/652 [04:18<00:00,  3.51it/s]\u001b[A\u001b[2;36m[02:20:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36mMetrics:\u001b[0m                                       \u001b]8;id=201629;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=738797;file:///home/thanhnx/bhc/utils/logger.py#205\u001b\\\u001b[2m205\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   eval_loss: \u001b[1;36m3.1501\u001b[0m                            \u001b]8;id=303445;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=83667;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   epoch: \u001b[1;36m0\u001b[0m                                     \u001b]8;id=291476;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=475435;file:///home/thanhnx/bhc/utils/logger.py#211\u001b\\\u001b[2m211\u001b[0m\u001b]8;;\u001b\\\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 3.1500697135925293, 'eval_runtime': 258.8965, 'eval_samples_per_second': 10.074, 'eval_steps_per_second': 2.518, 'eval_entropy': 1.7709290121961956, 'eval_num_tokens': 0.0, 'eval_mean_token_accuracy': 0.43494592926984915, 'epoch': 0}\n",
      "  0%|                                                    | 0/60 [04:19<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 652/652 [04:18<00:00,  3.51it/s]\u001b[A\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Evaluation at step \u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mepoch \u001b[1;36m0.00\u001b[0m\u001b[1m)\u001b[0m     \u001b]8;id=372528;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=219684;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#133\u001b\\\u001b[2m133\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36mMetrics:\u001b[0m                                       \u001b]8;id=716751;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=679514;file:///home/thanhnx/bhc/utils/logger.py#205\u001b\\\u001b[2m205\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   eval_loss: \u001b[1;36m3.1501\u001b[0m                            \u001b]8;id=560086;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=764544;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   eval_runtime: \u001b[1;36m258.8965\u001b[0m                       \u001b]8;id=283060;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=970342;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   eval_samples_per_second: \u001b[1;36m10.0740\u001b[0m             \u001b]8;id=717870;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=340035;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   eval_steps_per_second: \u001b[1;36m2.5180\u001b[0m                \u001b]8;id=240174;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=861722;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   eval_entropy: \u001b[1;36m1.7709\u001b[0m                         \u001b]8;id=280746;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=69403;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   eval_num_tokens: \u001b[1;36m0.0000\u001b[0m                      \u001b]8;id=918938;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=752787;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   eval_mean_token_accuracy: \u001b[1;36m0.4349\u001b[0m             \u001b]8;id=414850;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=927657;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   epoch: \u001b[1;36m0\u001b[0m                                     \u001b]8;id=277746;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146413;file:///home/thanhnx/bhc/utils/logger.py#211\u001b\\\u001b[2m211\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generating \u001b[1;36m5\u001b[0m sample predictions at    \u001b]8;id=275504;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=783300;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#156\u001b\\\u001b[2m156\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         step \u001b[1;36m0\u001b[0m\u001b[33m...\u001b[0m                             \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:20:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m1\u001b[0m prediction generated         \u001b]8;id=418801;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=379580;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1558\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:21:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m2\u001b[0m prediction generated         \u001b]8;id=95325;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=792495;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1616\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:21:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m3\u001b[0m prediction generated         \u001b]8;id=657924;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=167753;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1644\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:21:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m4\u001b[0m prediction generated         \u001b]8;id=66613;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=403457;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1553\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:22:23]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sample \u001b[1;36m5\u001b[0m prediction generated         \u001b]8;id=263626;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=580099;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         \u001b[1m(\u001b[0m\u001b[33mlen\u001b[0m=\u001b[1;36m1479\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m[02:22:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Logged \u001b[1;36m5\u001b[0m sample predictions to wandb  \u001b]8;id=755731;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=120116;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#208\u001b\\\u001b[2m208\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         at step \u001b[1;36m0\u001b[0m                             \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Saved predictions to                  \u001b]8;id=279786;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=805934;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#216\u001b\\\u001b[2m216\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         output/qwen_finetune/predictions_step \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m         _0.json                               \u001b[2m                      \u001b[0m\n",
      "\n",
      "                                                                                \u001b[A\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m1\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=165840;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=475763;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      "/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1154: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "  3%|‚ñà‚ñé                                       | 2/60 [06:56<2:47:41, 173.47s/it]\u001b[2;36m[02:22:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m1\u001b[0m completed. Total time: \u001b[1;36m9.06\u001b[0m   \u001b]8;id=524902;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=798975;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m1\u001b[0m completed.                    \u001b]8;id=655674;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=312942;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m2\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=208573;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=160263;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      "  7%|‚ñà‚ñà‚ñä                                       | 4/60 [07:28<1:02:43, 67.20s/it]\u001b[2;36m[02:23:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m2\u001b[0m completed. Total time: \u001b[1;36m9.59\u001b[0m   \u001b]8;id=999816;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=816449;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m2\u001b[0m completed.                    \u001b]8;id=628038;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=339902;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m3\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=380612;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=921406;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 6/60 [07:59<33:21, 37.06s/it]\u001b[2;36m[02:24:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m3\u001b[0m completed. Total time: \u001b[1;36m10.11\u001b[0m  \u001b]8;id=60738;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=252572;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m3\u001b[0m completed.                    \u001b]8;id=89814;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=767460;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m4\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=558582;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=803035;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                      | 8/60 [08:31<21:43, 25.07s/it]\u001b[2;36m[02:24:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m4\u001b[0m completed. Total time: \u001b[1;36m10.63\u001b[0m  \u001b]8;id=992842;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=576510;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m4\u001b[0m completed.                    \u001b]8;id=636059;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=443692;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m5\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=765388;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=723378;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 10/60 [09:02<16:27, 19.74s/it]\u001b[2;36m[02:25:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m5\u001b[0m completed. Total time: \u001b[1;36m11.16\u001b[0m  \u001b]8;id=704314;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=681446;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m5\u001b[0m completed.                    \u001b]8;id=473417;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=126882;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m6\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=22056;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=616886;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 12/60 [09:33<13:47, 17.24s/it]\u001b[2;36m[02:25:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m6\u001b[0m completed. Total time: \u001b[1;36m11.68\u001b[0m  \u001b]8;id=7540;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=74441;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m6\u001b[0m completed.                    \u001b]8;id=70674;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=949401;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m7\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=539131;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=249565;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 14/60 [10:05<12:23, 16.15s/it]\u001b[2;36m[02:26:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m7\u001b[0m completed. Total time: \u001b[1;36m12.21\u001b[0m  \u001b]8;id=565427;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=138739;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m7\u001b[0m completed.                    \u001b]8;id=604201;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=495631;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m8\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=426833;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=199659;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 16/60 [10:37<11:27, 15.63s/it]\u001b[2;36m[02:26:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m8\u001b[0m completed. Total time: \u001b[1;36m12.73\u001b[0m  \u001b]8;id=371507;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=444154;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m8\u001b[0m completed.                    \u001b]8;id=56802;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=706073;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m9\u001b[0m/\u001b[1;36m30\u001b[0m                   \u001b]8;id=422179;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=763587;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 18/60 [11:08<10:44, 15.35s/it]\u001b[2;36m[02:27:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m9\u001b[0m completed. Total time: \u001b[1;36m13.26\u001b[0m  \u001b]8;id=260735;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=200896;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m9\u001b[0m completed.                    \u001b]8;id=442374;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=192401;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m10\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=968114;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=79046;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 20/60 [11:40<10:05, 15.13s/it]\u001b[2;36m[02:27:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m10\u001b[0m completed. Total time: \u001b[1;36m13.78\u001b[0m \u001b]8;id=577057;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=102664;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m10\u001b[0m completed.                   \u001b]8;id=15474;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=97793;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m11\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=174389;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=426156;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 22/60 [12:12<09:33, 15.09s/it]\u001b[2;36m[02:28:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m11\u001b[0m completed. Total time: \u001b[1;36m14.32\u001b[0m \u001b]8;id=420521;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=946279;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m11\u001b[0m completed.                   \u001b]8;id=409386;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=278085;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m12\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=299105;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=443555;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 24/60 [12:43<09:00, 15.00s/it]\u001b[2;36m[02:28:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m12\u001b[0m completed. Total time: \u001b[1;36m14.84\u001b[0m \u001b]8;id=694022;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=753305;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m12\u001b[0m completed.                   \u001b]8;id=228275;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=61324;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m13\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=784309;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=328838;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 26/60 [13:15<08:30, 15.02s/it]\u001b[2;36m[02:29:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m13\u001b[0m completed. Total time: \u001b[1;36m15.37\u001b[0m \u001b]8;id=527276;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=964047;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m13\u001b[0m completed.                   \u001b]8;id=532496;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=84002;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m14\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=71262;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=708011;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 28/60 [13:46<08:01, 15.05s/it]\u001b[2;36m[02:29:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m14\u001b[0m completed. Total time: \u001b[1;36m15.90\u001b[0m \u001b]8;id=987335;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=933533;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m14\u001b[0m completed.                   \u001b]8;id=41672;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=649468;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m15\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=592683;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=548177;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 30/60 [14:18<07:31, 15.05s/it]\u001b[2;36m[02:30:18]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m15\u001b[0m completed. Total time: \u001b[1;36m16.42\u001b[0m \u001b]8;id=702258;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=750981;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m15\u001b[0m completed.                   \u001b]8;id=137235;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=704318;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m16\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=974146;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=788387;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 32/60 [14:50<07:01, 15.06s/it]\u001b[2;36m[02:30:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m16\u001b[0m completed. Total time: \u001b[1;36m16.95\u001b[0m \u001b]8;id=651332;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=590341;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m16\u001b[0m completed.                   \u001b]8;id=530458;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=278082;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m17\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=72132;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=921981;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 34/60 [15:21<06:29, 14.96s/it]\u001b[2;36m[02:31:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m17\u001b[0m completed. Total time: \u001b[1;36m17.48\u001b[0m \u001b]8;id=459469;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=874244;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m17\u001b[0m completed.                   \u001b]8;id=846305;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=685743;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m18\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=581542;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=313921;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 36/60 [15:53<05:59, 14.99s/it]\u001b[2;36m[02:31:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m18\u001b[0m completed. Total time: \u001b[1;36m18.00\u001b[0m \u001b]8;id=920782;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=140814;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m18\u001b[0m completed.                   \u001b]8;id=778480;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=580097;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m19\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=220861;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=752470;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 38/60 [16:24<05:29, 14.98s/it]\u001b[2;36m[02:32:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m19\u001b[0m completed. Total time: \u001b[1;36m18.53\u001b[0m \u001b]8;id=894343;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=276807;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m19\u001b[0m completed.                   \u001b]8;id=952273;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=887204;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m20\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=869634;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=290120;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 40/60 [16:56<05:00, 15.03s/it]\u001b[2;36m[02:32:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m20\u001b[0m completed. Total time: \u001b[1;36m19.05\u001b[0m \u001b]8;id=137174;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=668061;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m20\u001b[0m completed.                   \u001b]8;id=578478;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=739945;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m21\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=78898;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=990957;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 42/60 [17:27<04:31, 15.06s/it]\u001b[2;36m[02:33:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m21\u001b[0m completed. Total time: \u001b[1;36m19.58\u001b[0m \u001b]8;id=572092;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=37778;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m21\u001b[0m completed.                   \u001b]8;id=155287;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=450664;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m22\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=942650;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=978413;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 44/60 [17:59<04:01, 15.11s/it]\u001b[2;36m[02:34:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m22\u001b[0m completed. Total time: \u001b[1;36m20.11\u001b[0m \u001b]8;id=375190;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=220281;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m22\u001b[0m completed.                   \u001b]8;id=370858;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=818011;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m23\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=650810;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=785884;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 46/60 [18:31<03:31, 15.09s/it]\u001b[2;36m[02:34:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m23\u001b[0m completed. Total time: \u001b[1;36m20.64\u001b[0m \u001b]8;id=906604;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=170395;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m23\u001b[0m completed.                   \u001b]8;id=432322;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=25990;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m24\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=820391;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=976031;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 48/60 [19:02<03:01, 15.11s/it]\u001b[2;36m[02:35:03]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m24\u001b[0m completed. Total time: \u001b[1;36m21.16\u001b[0m \u001b]8;id=770763;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=850132;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m24\u001b[0m completed.                   \u001b]8;id=735343;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=113346;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m25\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=493554;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=233238;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 50/60 [19:34<02:30, 15.05s/it]\u001b[2;36m[02:35:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36mMetrics:\u001b[0m                                       \u001b]8;id=366643;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=320015;file:///home/thanhnx/bhc/utils/logger.py#205\u001b\\\u001b[2m205\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   loss: \u001b[1;36m2.6337\u001b[0m                                 \u001b]8;id=233752;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=24813;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   learning_rate: \u001b[1;36m0.0000\u001b[0m                        \u001b]8;id=292136;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=906339;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   epoch: \u001b[1;36m25.0000\u001b[0m                               \u001b]8;id=672642;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=534150;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "{'loss': 2.6337, 'grad_norm': 0.1110353097319603, 'learning_rate': 4.946920181123904e-05, 'entropy': 2.449707835960388, 'num_tokens': 1901025.0, 'mean_token_accuracy': 0.4782743694782257, 'epoch': 25.0}\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 50/60 [19:34<02:30, 15.05s/it]\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m25\u001b[0m completed. Total time: \u001b[1;36m21.69\u001b[0m \u001b]8;id=347235;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=984861;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m25\u001b[0m completed.                   \u001b]8;id=187241;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=608792;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m26\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=455673;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=362479;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 52/60 [20:06<01:59, 14.99s/it]\u001b[2;36m[02:36:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m26\u001b[0m completed. Total time: \u001b[1;36m22.22\u001b[0m \u001b]8;id=635656;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=536265;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m26\u001b[0m completed.                   \u001b]8;id=199312;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=267095;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m27\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=545175;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=970535;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 54/60 [20:37<01:30, 15.10s/it]\u001b[2;36m[02:36:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m27\u001b[0m completed. Total time: \u001b[1;36m22.74\u001b[0m \u001b]8;id=985937;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=777991;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m27\u001b[0m completed.                   \u001b]8;id=452248;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=73372;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m28\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=653425;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=329164;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 56/60 [21:09<01:00, 15.11s/it]\u001b[2;36m[02:37:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m28\u001b[0m completed. Total time: \u001b[1;36m23.27\u001b[0m \u001b]8;id=943767;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=314910;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m28\u001b[0m completed.                   \u001b]8;id=342027;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=421947;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m29\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=201158;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=440869;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 58/60 [21:40<00:30, 15.08s/it]\u001b[2;36m[02:37:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m29\u001b[0m completed. Total time: \u001b[1;36m23.80\u001b[0m \u001b]8;id=784475;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=946660;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m29\u001b[0m completed.                   \u001b]8;id=425800;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=574553;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting epoch \u001b[1;36m30\u001b[0m/\u001b[1;36m30\u001b[0m                  \u001b]8;id=220392;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=450770;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#295\u001b\\\u001b[2m295\u001b[0m\u001b]8;;\u001b\\\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [22:12<00:00, 15.14s/it]Saving model checkpoint to output/qwen_finetune/checkpoint-60\n",
      "loading configuration file config.json from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6144,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "chat template saved in output/qwen_finetune/checkpoint-60/chat_template.jinja\n",
      "tokenizer config file saved in output/qwen_finetune/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in output/qwen_finetune/checkpoint-60/special_tokens_map.json\n",
      "\u001b[2;36m[02:38:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m30\u001b[0m completed. Total time: \u001b[1;36m24.38\u001b[0m \u001b]8;id=337902;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=487575;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#300\u001b\\\u001b[2m300\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         min                                   \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Epoch \u001b[1;36m30\u001b[0m completed.                   \u001b]8;id=536004;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=496171;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36mMetrics:\u001b[0m                                       \u001b]8;id=177934;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=690855;file:///home/thanhnx/bhc/utils/logger.py#205\u001b\\\u001b[2m205\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   train_loss: \u001b[1;36m2.5930\u001b[0m                           \u001b]8;id=663686;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=649342;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m   epoch: \u001b[1;36m30.0000\u001b[0m                               \u001b]8;id=787625;file:///home/thanhnx/bhc/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=246277;file:///home/thanhnx/bhc/utils/logger.py#209\u001b\\\u001b[2m209\u001b[0m\u001b]8;;\u001b\\\n",
      "{'train_runtime': 1462.9509, 'train_samples_per_second': 2.051, 'train_steps_per_second': 0.041, 'train_loss': 2.593024984995524, 'entropy': 2.3690838356018067, 'num_tokens': 2281230.0, 'mean_token_accuracy': 0.5075289108753205, 'epoch': 30.0}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [22:16<00:00, 15.14s/it]\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Training completed! Total time: \u001b[1;36m24.38\u001b[0m \u001b]8;id=154511;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=25611;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#317\u001b\\\u001b[2m317\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m         minutes                               \u001b[2m                      \u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [22:16<00:00, 22.27s/it]\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Saving final model\u001b[33m...\u001b[0m                 \u001b]8;id=805819;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py\u001b\\\u001b[2mfine_tune_llama.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=76365;file:///home/thanhnx/bhc/summarization/fine_tune_llama.py#754\u001b\\\u001b[2m754\u001b[0m\u001b]8;;\u001b\\\n",
      "Saving model checkpoint to output/qwen_finetune/best_val_loss\n",
      "loading configuration file config.json from cache at /home/thanhnx/.cache/huggingface/hub/models--Qwen--Qwen3-1.7B/snapshots/70d244cc86ccca08cf5af4e1e306ecf908b1ad5e/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 6144,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thanhnx/bhc/summarization/fine_tune_llama.py\", line 837, in <module>\n",
      "    main()\n",
      "  File \"/home/thanhnx/bhc/summarization/fine_tune_llama.py\", line 755, in main\n",
      "    trainer.save_model(f\"{output_dir}/best_val_loss\")\n",
      "  File \"/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/transformers/trainer.py\", line 4227, in save_model\n",
      "    self._save(output_dir)\n",
      "  File \"/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/transformers/trainer.py\", line 4331, in _save\n",
      "    self.model.save_pretrained(\n",
      "  File \"/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/peft/peft_model.py\", line 331, in save_pretrained\n",
      "    safe_save_file(\n",
      "  File \"/home/thanhnx/.conda/envs/all/lib/python3.11/site-packages/safetensors/torch.py\", line 307, in save_file\n",
      "    serialize_file(_flatten(tensors), filename, metadata=metadata)\n",
      "safetensors_rust.SafetensorError: Error while serializing: I/O error: No space left on device (os error 28)\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mlikely-star-3\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251209_021336-pbdegmhr/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!./run_finetune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55370e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter from: output/qwen_finetune/checkpoint-60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fa8e85d3054c72ab70e3e05f91e069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized embeddings to 151669 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Generating summary...\n",
      "================================================================================\n",
      "REFERENCE SUMMARY:\n",
      "You were admitted to the ___ for management of your right leg cellulitis. You were started on IV antibiotics on your admission and transitioned to oral antibiotics throughout your stay. Your cellulitis has improved throughout your stay. We also explored your mixed symptoms of nausea, difficulty swallowing and epigastric pain. We spoke with the GI team who performed an upper endoscopy on ___. This study found 2 small ulcers in your duodenum as the likely cause of your symptoms. These kinds of ulcers are almost always caused by a bacterial infection called H. Pylori. We sent your stool off for studies to confirm this diagnosis and started you on a PPI call Omeprazole, a medication that helps with the symptoms you have been experiencing. Please take this medication two times per day, 30 minutes before meals.\n",
      "================================================================================\n",
      "MODEL PREDICTION:\n",
      "1. You were admitted to the hospital because of a severe infection in your leg. We treated you with IV antibiotics and you were discharged home with a course of oral antibiotics. 2. You were seen by the vascular surgery service and we monitored your leg for improvement. 3. You were seen by the GI service and we injected your duodenal ulcers with Epinephrine and cauterized them. 4. You were discharged home with a PPI and a course of oral antibiotics. 5. You were discharged home with a regular diet and with services. 6. You were discharged home with a follow up appointment with your PCP in ___ to discuss the results of your H. pylori test. 7. You were discharged home with a follow up appointment with your PCP in ___ to discuss the results of your H. pylori test. 8. You were discharged home with a regular diet and with services. 9. You were discharged home with a regular diet and with services. 10. You were discharged home with a regular diet and with services. 11. You were discharged home with a regular diet and with services. 12. You were discharged home with a regular diet and with services. 13. You were discharged home with a regular diet and with services. 14. You were discharged home with a regular diet and with services. 15. You were discharged home with a regular diet and with services. 16. You were discharged home with a regular diet and with services. 17. You were discharged home with a regular diet and with services. 18. You were discharged home with a regular diet and with services\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Inference with Best Checkpoint\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "base_model_name = 'Qwen/Qwen3-1.7B'\n",
    "output_dir = 'output/qwen_finetune'\n",
    "\n",
    "# Find the best checkpoint (latest checkpoint-* folder)\n",
    "checkpoints = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "if checkpoints:\n",
    "    # Sort by step number and get the latest\n",
    "    checkpoints.sort(key=lambda x: int(x.split('-')[1]))\n",
    "    adapter_path = os.path.join(output_dir, checkpoints[-1])\n",
    "else:\n",
    "    adapter_path = os.path.join(output_dir, 'best_val_loss')\n",
    "\n",
    "test_file = 'dataset/mimic-iv-note-ext-di-bhc/dataset/test_4000_600_chars_last_100.json'\n",
    "\n",
    "print(f'Loading adapter from: {adapter_path}')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load Base Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
    "\n",
    "# Resize embeddings to match checkpoint (important!)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f'Resized embeddings to {len(tokenizer)} tokens')\n",
    "\n",
    "# Load LoRA Adapter\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model.eval()\n",
    "print('Model loaded successfully!')\n",
    "\n",
    "# Load a sample from test set\n",
    "with open(test_file, 'r') as f:\n",
    "    line = f.readline()\n",
    "    data = json.loads(line)\n",
    "    sample_text = data['text'] ##thay thanh text t√πy √Ω\n",
    "    reference = data['summary']\n",
    "\n",
    "# Prepare Prompt\n",
    "instruction = \"Summarize for the patient what happened during the hospital stay based on this doctor's note:\"\n",
    "response_prefix = \"Summary for the patient:\"\n",
    "prompt = f\"{instruction}\\n{sample_text}\\n{response_prefix} \"\n",
    "\n",
    "# Generate\n",
    "print('Generating summary...')\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        max_new_tokens=350,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract only the generated part after the response prefix\n",
    "if response_prefix in prediction:\n",
    "    prediction = prediction.split(response_prefix)[-1].strip()\n",
    "\n",
    "print('=' * 80)\n",
    "print('REFERENCE SUMMARY:')\n",
    "print(reference)\n",
    "print('=' * 80)\n",
    "print('MODEL PREDICTION:')\n",
    "print(prediction)\n",
    "print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba69d17",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Gradio Demo: Baseline vs Fine-tuned Model Comparison\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\bhcnlp\\lib\\site-packages\\transformers\\__init__.py:958\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m    956\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 958\u001b[0m import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})]\u001b[38;5;241m.\u001b[39mupdate(_import_structure)\n\u001b[0;32m    961\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m _LazyModule(\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m     extra_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m: __version__},\n\u001b[0;32m    967\u001b[0m )\n",
      "File \u001b[1;32md:\\miniconda\\envs\\bhcnlp\\lib\\site-packages\\transformers\\utils\\import_utils.py:2867\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[1;34m(module_path, prefix)\u001b[0m\n\u001b[0;32m   2843\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[0;32m   2844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \u001b[38;5;124;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[0;32m   2847\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2865\u001b[0m \u001b[38;5;124;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[0;32m   2866\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2867\u001b[0m     import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2868\u001b[0m     spread_dict \u001b[38;5;241m=\u001b[39m spread_import_structure(import_structure)\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\miniconda\\envs\\bhcnlp\\lib\\site-packages\\transformers\\utils\\import_utils.py:2580\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(module_path):\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__pycache__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(module_path, f)):\n\u001b[1;32m-> 2580\u001b[0m         import_structure[f] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2582\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, f)):\n\u001b[0;32m   2583\u001b[0m         adjacent_modules\u001b[38;5;241m.\u001b[39mappend(f)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\bhcnlp\\lib\\site-packages\\transformers\\utils\\import_utils.py:2604\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2602\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 2604\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   2605\u001b[0m     file_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\bhcnlp\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gradio Demo: Baseline vs Fine-tuned Model Comparison\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "base_model_name = 'Qwen/Qwen3-1.7B'\n",
    "output_dir = 'output/qwen_finetune'\n",
    "\n",
    "# Find the best checkpoint\n",
    "checkpoints = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "if checkpoints:\n",
    "    checkpoints.sort(key=lambda x: int(x.split('-')[1]))\n",
    "    adapter_path = os.path.join(output_dir, checkpoints[-1])\n",
    "else:\n",
    "    adapter_path = os.path.join(output_dir, 'best_val_loss')\n",
    "\n",
    "print(f'Adapter path: {adapter_path}')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load Baseline Model\n",
    "print('Loading baseline model...')\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "baseline_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "baseline_model.eval()\n",
    "print('Baseline model loaded!')\n",
    "\n",
    "# Load Fine-tuned Model\n",
    "print('Loading fine-tuned model...')\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
    "finetuned_model.resize_token_embeddings(len(finetuned_tokenizer))\n",
    "finetuned_model = PeftModel.from_pretrained(finetuned_model, adapter_path)\n",
    "finetuned_model.eval()\n",
    "print('Fine-tuned model loaded!')\n",
    "\n",
    "# Prompt template\n",
    "instruction = \"Summarize for the patient what happened during the hospital stay based on this doctor's note:\"\n",
    "response_prefix = \"Summary for the patient:\"\n",
    "\n",
    "def generate_summary(clinical_note, model, tokenizer, max_tokens=350):\n",
    "    prompt = f\"{instruction}\\n{clinical_note}\\n{response_prefix} \"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if response_prefix in prediction:\n",
    "        prediction = prediction.split(response_prefix)[-1].strip()\n",
    "    return prediction\n",
    "\n",
    "def compare_models(clinical_note):\n",
    "    if not clinical_note.strip():\n",
    "        return \"Please enter a clinical note.\", \"Please enter a clinical note.\"\n",
    "    \n",
    "    baseline_output = generate_summary(clinical_note, baseline_model, baseline_tokenizer)\n",
    "    finetuned_output = generate_summary(clinical_note, finetuned_model, finetuned_tokenizer)\n",
    "    \n",
    "    return baseline_output, finetuned_output\n",
    "\n",
    "# Load sample data for examples\n",
    "test_file = 'dataset/mimic-iv-note-ext-di-bhc/dataset/test_4000_600_chars_last_100.json'\n",
    "examples = []\n",
    "with open(test_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:  # Only load 3 examples\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        examples.append([data['text'][:2000]])  # Truncate for display\n",
    "\n",
    "# Create Gradio Interface\n",
    "with gr.Blocks(title=\"Medical Summarization: Baseline vs Fine-tuned\") as demo:\n",
    "    gr.Markdown(\"# üè• Medical Summarization Comparison\")\n",
    "    gr.Markdown(\"Compare **Baseline Qwen3-1.7B** vs **Fine-tuned on MIMIC-IV BHC**\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        input_text = gr.Textbox(\n",
    "            label=\"Clinical Note (Doctor's Brief Hospital Course)\",\n",
    "            placeholder=\"Enter the clinical note here...\",\n",
    "            lines=10\n",
    "        )\n",
    "    \n",
    "    compare_btn = gr.Button(\"üîç Compare Models\", variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### üìù Baseline Model Output\")\n",
    "            baseline_output = gr.Textbox(label=\"Baseline (Qwen3-1.7B)\", lines=8)\n",
    "        \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### ‚ú® Fine-tuned Model Output\")\n",
    "            finetuned_output = gr.Textbox(label=\"Fine-tuned (LoRA)\", lines=8)\n",
    "    \n",
    "    compare_btn.click(\n",
    "        fn=compare_models,\n",
    "        inputs=[input_text],\n",
    "        outputs=[baseline_output, finetuned_output]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"### üìã Example Clinical Notes\")\n",
    "    gr.Examples(\n",
    "        examples=examples,\n",
    "        inputs=[input_text]\n",
    "    )\n",
    "\n",
    "# Launch\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhcnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
